
<!DOCTYPE HTML>
<html>
<head>
  <title>Machine Learning January 2025 Unit 7: Perceptron Activities</title>
  <meta charset="utf-8" />
  <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body class="is-preload">
  <div id="wrapper">

    <!-- Header -->
    <header id="header">
      <a href="index.html" class="logo"><strong>Pavlos</strong> <span>e-Portfolio</span></a>
    </header>

    <!-- Main -->
    <div id="main" class="alt">
      <section class="inner">
        <header class="major">
          <h1>Machine Learning January 2025 Unit 7: Perceptron Activities</h1>
          <p>Exploring Perceptron models and preprocessing techniques in machine learning.</p>
        </header>

        <h2> Task Overview</h2>
        <p>This unit covers the foundations of Perceptron models, starting with simple structures and progressing to multi-layer neural networks. Tasks include preprocessing data, visualizations, and model training using Scikit-Learn.</p>

<h3> Task 7: Initial Analysis</h3>

<h4> a) Simple Perceptron â€“ Linearly Separable Classification</h4>
<p><strong>Description:</strong> This exercise introduced the basic perceptron algorithm and applied it to a linearly separable dataset using binary inputs.</p>
<p><strong>Method:</strong> A perceptron model was trained on synthetic binary input pairs. The model updated weights iteratively based on the prediction error until convergence.</p>
<p><strong>Results:</strong> The perceptron was able to separate the classes correctly, confirming its effectiveness on linearly separable problems.</p>
<ul>
  <li><a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/Unit07%20Ex1%20simple_perceptron.ipynb" target="_blank">ðŸ“¥ Download/View Notebook (.ipynb)</a></li>
  <li><a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/Unit07%20Ex1%20simple_perceptron.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></li>
</ul>

<h4> b) Perceptron with AND Operator â€“ Repetition and Generalization</h4>
<p><strong>Description:</strong> This task extended the basic perceptron to repeated AND logic inputs to test its learning stability and robustness.</p>
<p><strong>Method:</strong> Each unique binary input combination was repeated 25 times in the dataset. The perceptron was trained with the same update rule as before.</p>
<p><strong>Results:</strong> The model achieved perfect classification accuracy quickly, and the repeated structure led to faster convergence and better generalization.</p>
<ul>
  <li><a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/Unit07%20Ex2%20perceptron_AND_operator.ipynb" target="_blank">ðŸ“¥ Download/View Notebook (.ipynb)</a></li>
  <li><a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/Unit07%20Ex2%20perceptron_AND_operator.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"></a></li>
</ul>

<h4> c) Multi-layer Perceptron â€“ XOR Problem</h4>
<p><strong>Description:</strong> This final task introduced a simple MLP to solve the XOR classification problem, which is not linearly separable.</p>
<p><strong>Method:</strong> A neural network with one hidden layer and ReLU activation was implemented. The training dataset contained 100 randomized XOR samples.</p>
<p><strong>Results:</strong> The MLP successfully learned the correct classification by forming non-linear decision boundaries, overcoming the limitations of a simple perceptron.</p>
<ul>



        
        <h3> Access Notebooks for the furhter analysis</h3>
        <ul>
          <li>
            <strong>Task A: Simple Perceptron</strong><br>
            <a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/unit7a_with_analysis.ipynb" target="_blank"> Download/View (.ipynb)</a>
            |
            <a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/unit7a_with_analysis.ipynb" target="_blank">
              <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
            </a>
          </li>
          <li>
            <strong>Task B: Perceptron AND Operator</strong><br>
            <a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/unit7b_with_analysis.ipynb" target="_blank"> Download/View (.ipynb)</a>
            |
            <a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/unit7b_with_analysis.ipynb" target="_blank">
              <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
            </a>
          </li>
          <li>
            <strong>Task C: Multi-layer Perceptron</strong><br>
            <a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/unit7c_with_analysis.ipynb" target="_blank"> Download/View (.ipynb)</a>
            |
            <a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/unit7c_with_analysis.ipynb" target="_blank">
              <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
            </a>
          </li>
        </ul>

        
<h3> Results</h3>

<h4> a) Simple Perceptron</h4>
<p><strong>Method:</strong> We trained a simple Perceptron on a dataset of 100 synthetic binary input pairs, where the target label followed an AND-like logic (output is 1 only if both inputs are 1). The dataset was split into training and test sets.</p>
<p><strong>Results:</strong> The perceptron learned to separate the classes with a high degree of accuracy. The test set confirmed the modelâ€™s ability to draw a linear decision boundary between the classes.</p>
<p><strong>Interpretation:</strong> This shows that a single-layer perceptron can successfully solve linearly separable problems by learning from labeled data through iterative weight updates.</p>

  

<h4> b) Perceptron AND Operator</h4>
<p><strong>Method:</strong> A Perceptron was trained using repeated AND gate logic (inputs repeated 25 times). This ensured balanced exposure to all binary input combinations for a linearly separable classification task.</p>
<p><strong>Results:</strong> The perceptron achieved perfect classification on the repeated AND dataset. The modelâ€™s weights stabilized quickly and produced consistent predictions.</p>
<p><strong>Interpretation:</strong> This reinforces the strength of perceptrons in solving simple logic problems that are linearly separable. The experiment also highlighted how data repetition aids model generalization.</p>


<h4> c) Multi-layer Perceptron (MLP)</h4>
<p><strong>Method:</strong> An MLP was trained on an XOR dataset (non-linearly separable). A single hidden layer with ReLU activation was used. Data was expanded to 100 examples to ensure robust training.</p>
<p><strong>Results:</strong> The MLP reached perfect accuracy on the XOR task. The hidden layer enabled it to model complex decision boundaries that a simple perceptron cannot learn.</p>
<p><strong>Interpretation:</strong> This experiment demonstrates how MLPs overcome the limitation of linear separability, making them suitable for real-world, complex classification tasks where simple decision surfaces aren't sufficient.</p>

<!-- Difference Between Perceptron and Regression:
Perceptron:
- Binary classification model.
- Uses a step function (activation) to decide classes.
- Weight updates driven by misclassification (error-based learning).

Regression:
- Predicts continuous outcomes.
- Uses least squares minimization or similar loss functions.
- No activation function â€” output is direct numerical value.

Key takeaway: Perceptron classifies; regression predicts a continuous value.
-->

        <p><strong>Task A:</strong> Skewness (0.28) indicated slight right skew. Kurtosis (-1.26) suggested a flat, wide distribution. Perceptron maintained stable convergence.</p>
        <p><strong>Task B:</strong> Perceptron AND logic had near-zero skew (0.23) and flat kurtosis (-1.08). Well-suited for linear decision boundaries.</p>
        <p><strong>Task C:</strong> Multi-layer perceptron showed balanced data input (skew: 0.12). The model benefitted from deeper structure and activation functions.</p>
        <p><strong>Additional:</strong> All tasks used a population of 20 random values between -100 and +100. Results showed consistency in model behavior across scenarios.</p>
        <!-- Difference Between Perceptron and Regression:
        Perceptron:
        - Binary classification model.
        - Uses a step function (activation) to decide classes.
        - Weight updates driven by misclassification (error-based learning).

        Regression:
        - Predicts continuous outcomes.
        - Uses least squares minimization or similar loss functions.
        - No activation function â€” output is direct numerical value.

        Key takeaway: Perceptron classifies; regression predicts a continuous value.
        -->

        
<h3>Highlights & Reflections</h3>

<h4> Key Takeaways</h4>
<ul>
  <li><strong>Simple Perceptrons</strong> are effective for solving <em>linearly separable problems</em>, as demonstrated by the success on the basic AND logic task.</li>
  <li><strong>Repetition of input patterns</strong> (as in the second experiment) plays a vital role in enhancing model stability and generalization. By repeating AND gate inputs, the perceptron learned faster and more consistently.</li>
  <li>The <strong>Multi-layer Perceptron (MLP)</strong> showed clear superiority when dealing with <em>non-linearly separable data</em> like XOR. The hidden layer enabled the model to learn complex patterns that simple perceptrons could not grasp.</li>
</ul>

<h4> Reflections</h4>
<p>Through these experiments, the progression from a simple perceptron to a more advanced MLP illuminated the importance of model architecture in machine learning. While the simplicity of a single-layer network is powerful within its limits, real-world problems often demand the flexibility and representational power of deeper models.</p>
<p>Moreover, the exercises reinforced a key learning principle: <strong>data structure and representation matter</strong>. Whether it's the linear separability of a dataset or the effect of repeated input exposure, understanding the data is as crucial as selecting the right model.</p>
<p>These foundational experiments build a strong conceptual bridge toward exploring more sophisticated neural networks in future units.</p>


      
      </section>
    </div>
  </div>
</body>
</html>
