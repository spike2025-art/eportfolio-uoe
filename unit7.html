
<!DOCTYPE HTML>
<html>
<head>
  <title>Machine Learning January 2025 Unit 7: Perceptron Activities</title>
  <meta charset="utf-8" />
  <link rel="stylesheet" href="assets/css/main.css" />
</head>
<body class="is-preload">
  <div id="wrapper">

    <!-- Header -->
    <header id="header">
      <a href="index.html" class="logo"><strong>Pavlos</strong> <span>e-Portfolio</span></a>
    </header>

    <!-- Main -->
    <div id="main" class="alt">
      <section class="inner">
        <header class="major">
          <h1>Machine Learning January 2025 Unit 7: Perceptron Activities</h1>
          <p>Exploring Perceptron models and preprocessing techniques in machine learning.</p>
        </header>

        <h2> Task Overview</h2>
        <p>This unit covers the foundations of Perceptron models, starting with simple structures and progressing to multi-layer neural networks. Tasks include preprocessing data, visualizations, and model training using Scikit-Learn.</p>

        <h3> Access Notebooks</h3>
        <ul>
          <li>
            <strong>Task A: Simple Perceptron</strong><br>
            <a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/unit7a_with_analysis.ipynb" target="_blank"> Download/View (.ipynb)</a>
            |
            <a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/unit7a_with_analysis.ipynb" target="_blank">
              <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
            </a>
          </li>
          <li>
            <strong>Task B: Perceptron AND Operator</strong><br>
            <a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/unit7b_with_analysis.ipynb" target="_blank"> Download/View (.ipynb)</a>
            |
            <a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/unit7b_with_analysis.ipynb" target="_blank">
              <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
            </a>
          </li>
          <li>
            <strong>Task C: Multi-layer Perceptron</strong><br>
            <a href="https://github.com/spike2025-art/eportfolio-uoe/raw/main/unit7c_with_analysis.ipynb" target="_blank"> Download/View (.ipynb)</a>
            |
            <a href="https://colab.research.google.com/github/spike2025-art/eportfolio-uoe/blob/main/unit7c_with_analysis.ipynb" target="_blank">
              <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab">
            </a>
          </li>
        </ul>

        <h3> Highlights</h3>
        <ul>
          <li>Identified missing values and handled them with imputation.</li>
          <li>Calculated skewness and kurtosis to understand data distribution.</li>
          <li>Generated heatmaps and scatter plots for correlation insights.</li>
          <li>Replaced categorical data with numeric encoding for modeling.</li>
          <li>Built and trained Perceptrons on basic AND logic and real data.</li>
        </ul>

        
<h3> Results</h3>

<h4> a) Simple Perceptron</h4>
<p><strong>Method:</strong> We trained a simple Perceptron on a dataset of 100 synthetic binary input pairs, where the target label followed an AND-like logic (output is 1 only if both inputs are 1). The dataset was split into training and test sets.</p>
<p><strong>Results:</strong> The perceptron learned to separate the classes with a high degree of accuracy. The test set confirmed the model’s ability to draw a linear decision boundary between the classes.</p>
<p><strong>Interpretation:</strong> This shows that a single-layer perceptron can successfully solve linearly separable problems by learning from labeled data through iterative weight updates.</p>

  

<h4> b) Perceptron AND Operator</h4>
<p><strong>Method:</strong> A Perceptron was trained using repeated AND gate logic (inputs repeated 25 times). This ensured balanced exposure to all binary input combinations for a linearly separable classification task.</p>
<p><strong>Results:</strong> The perceptron achieved perfect classification on the repeated AND dataset. The model’s weights stabilized quickly and produced consistent predictions.</p>
<p><strong>Interpretation:</strong> This reinforces the strength of perceptrons in solving simple logic problems that are linearly separable. The experiment also highlighted how data repetition aids model generalization.</p>


<h4> c) Multi-layer Perceptron (MLP)</h4>
<p><strong>Method:</strong> An MLP was trained on an XOR dataset (non-linearly separable). A single hidden layer with ReLU activation was used. Data was expanded to 100 examples to ensure robust training.</p>
<p><strong>Results:</strong> The MLP reached perfect accuracy on the XOR task. The hidden layer enabled it to model complex decision boundaries that a simple perceptron cannot learn.</p>
<p><strong>Interpretation:</strong> This experiment demonstrates how MLPs overcome the limitation of linear separability, making them suitable for real-world, complex classification tasks where simple decision surfaces aren't sufficient.</p>

<!-- Difference Between Perceptron and Regression:
Perceptron:
- Binary classification model.
- Uses a step function (activation) to decide classes.
- Weight updates driven by misclassification (error-based learning).

Regression:
- Predicts continuous outcomes.
- Uses least squares minimization or similar loss functions.
- No activation function — output is direct numerical value.

Key takeaway: Perceptron classifies; regression predicts a continuous value.
-->

        <p><strong>Task A:</strong> Skewness (0.28) indicated slight right skew. Kurtosis (-1.26) suggested a flat, wide distribution. Perceptron maintained stable convergence.</p>
        <p><strong>Task B:</strong> Perceptron AND logic had near-zero skew (0.23) and flat kurtosis (-1.08). Well-suited for linear decision boundaries.</p>
        <p><strong>Task C:</strong> Multi-layer perceptron showed balanced data input (skew: 0.12). The model benefitted from deeper structure and activation functions.</p>
        <p><strong>Additional:</strong> All tasks used a population of 20 random values between -100 and +100. Results showed consistency in model behavior across scenarios.</p>
        <!-- Difference Between Perceptron and Regression:
        Perceptron:
        - Binary classification model.
        - Uses a step function (activation) to decide classes.
        - Weight updates driven by misclassification (error-based learning).

        Regression:
        - Predicts continuous outcomes.
        - Uses least squares minimization or similar loss functions.
        - No activation function — output is direct numerical value.

        Key takeaway: Perceptron classifies; regression predicts a continuous value.
        -->

        
<h3> Reflection</h3>
<p><strong>What?</strong> We implemented and trained both simple and multi-layer perceptrons, incorporating synthetic and real data. Preprocessing steps like encoding and visual analysis were also covered.</p>
<p><strong>So What?</strong> This unit reinforced critical neural network concepts such as weight updates, linear separability, and hidden layer structure. Visualization and statistical analysis helped interpret input distributions.</p>
<p><strong>What Next?</strong> These learnings will support the development of more complex classifiers in later modules or industry projects.</p>
<p><strong>Key Concept – Perceptron vs Regression:</strong> While regression estimates continuous values based on trends, perceptrons are designed to classify inputs into categories. Perceptrons use an activation function and are influenced by misclassification, whereas regressions optimize a loss function without classification boundaries.</p>

        <p><strong>What?</strong> We implemented and trained simple and multi-layer Perceptrons and learned how to prepare data effectively.</p>
        <p><strong>So What?</strong> This practice deepened understanding of how neural network models process data, how input encoding affects performance, and the significance of learning rates and iteration settings.</p>
        <p><strong>What Next?</strong> These insights are directly applicable to more complex classification problems, and provide a strong foundation for neural network modeling in areas such as fraud detection, credit scoring, and risk analytics.</p>
      </section>
    </div>
  </div>
</body>
</html>
